





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import dtreeviz

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report





df = pd.read_csv('customer_churn_data.csv')
df.head()


df.info()


df.isna().sum(), df.duplicated().sum()


sns.set_style("whitegrid")
plt.pie(df['Churn'].value_counts(), labels=['Yes', 'No'], autopct='%1.1f%%')
plt.title('Distribution of churn')
plt.tight_layout()








df = df.drop('CustomerID', axis=1)

X, y = df.drop('Churn', axis=1), df['Churn']

X = X.apply(
    lambda column: LabelEncoder().fit_transform(column) if column.dtype == 'object' else column
)

y = LabelEncoder().fit_transform(y)

X[:3], y[:3]


X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=101)





model = DecisionTreeClassifier(criterion='entropy', max_depth=8)
model.fit(X_train, y_train)


y_hat = model.predict(X_test)

print(classification_report(y_hat, y_test))





features_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance':  model.feature_importances_
})

plt.figure(figsize=(10, 6))
sns.barplot(
    data=features_df.sort_values(by='Importance', ascending=False),
    x='Importance',
    y='Feature'
)
plt.title('Feature Importance in Decision Tree', fontsize=16)
plt.show()


plt.figure(figsize=(15, 10))

plot_tree(model, feature_names=X.columns, class_names=['No', 'Yes',], filled=True, rounded=True, fontsize=10)
plt.title('The tree that our model has created')

plt.show()


viz = dtreeviz.model(
    model, X, y,
    target_name='Churn',
    feature_names=X.columns, 
    class_names=["No", "Yes"]
)
viz.view(scale=2, title='The tree that our model has created')



